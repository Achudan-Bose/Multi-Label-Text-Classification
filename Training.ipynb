{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnIdQ0ZXZGER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f684c01-1216-4de9-a1c4-f059c10d8b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "import os\n",
        "from tqdm import tqdm, trange\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from sklearn.metrics import f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "LMgfrwqPT7F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=torch.load('/content/drive/My Drive/tensor data/train_data.pt', weights_only=False)\n",
        "validation_data=torch.load('/content/drive/My Drive/tensor data/validation_data.pt', weights_only=False)"
      ],
      "metadata": {
        "id": "HYA5gJkDrSJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the train and validation tensor datasets"
      ],
      "metadata": {
        "id": "VX4niZ1eRvnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "\n",
        "train_sampler=RandomSampler(train_data)\n",
        "train_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_sampler=SequentialSampler(validation_data)\n",
        "validation_dataloader=DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "txLJ-u2qrQcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch size is a hyperparameter which defines how many samples the model can process at a time. After trial and error, 64 seemed appropriate.\n",
        "\n",
        "Train data is shuffled so that there is no patterns in the order of data.\n",
        "Validation data is sequential so that the evaluation is reproducible.\n",
        "\n",
        "Dataloaders are created which will provide batches of data from the tensor dataset."
      ],
      "metadata": {
        "id": "jWnVriqj5xxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"thomas-sounack/BioClinical-ModernBERT-base\", num_labels=14)\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fafeea01f64843bfae93ca7c9fcfe37a",
            "c378aaca35bb4a0097df66da4e449d98",
            "3582cba7878c4297a74be4aebb0b4b1a",
            "541f7b03361e459185f49b672f401ace",
            "dfb70ba85149483588b7cb9025cc7856",
            "65d5e73471d54453a59100406dbcdce8",
            "84bef7808c69412caee61c5b30e4dd17",
            "8cf3595d9b87475e82f804951ea85799",
            "20155fd00f924261bb194e135acc6c14",
            "0ab6c12f582c46ddac9cba34849f8345",
            "d2b2251729544bd3b66de7f1410a33da",
            "47c4d79fa10549aa907c51df718fcd1f",
            "a7645df1e1be4d93914e3eb136f8e038",
            "d09524a22dce4d65b1fd5800aeb92579",
            "cbd96859a2f84e2d8175e64c20d4c5bc",
            "bc6d3ecbaa74436487263f4aaf027336",
            "2d3ab5565c6c44b6bfb19378a5a0be3d",
            "da5f4c8135414c5aacd42251e0515637",
            "54a8e8a83f434a2295d3820d370e824c",
            "1fa6193b9af048c7af9295c12b1f9f56",
            "9772484678cc421e9a15f6afd500b744",
            "bcaa7262e29345c59ff83f6f0dbe5876"
          ]
        },
        "id": "k2f36Cy8T7IF",
        "outputId": "a117bb7a-2521-4689-e819-defeb0e918a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fafeea01f64843bfae93ca7c9fcfe37a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47c4d79fa10549aa907c51df718fcd1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at thomas-sounack/BioClinical-ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModernBertForSequenceClassification(\n",
              "  (model): ModernBertModel(\n",
              "    (embeddings): ModernBertEmbeddings(\n",
              "      (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
              "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): ModernBertEncoderLayer(\n",
              "        (attn_norm): Identity()\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "      (1-21): 21 x ModernBertEncoderLayer(\n",
              "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): ModernBertAttention(\n",
              "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (rotary_emb): ModernBertRotaryEmbedding()\n",
              "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (out_drop): Identity()\n",
              "        )\n",
              "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): ModernBertMLP(\n",
              "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (act): GELUActivation()\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (head): ModernBertPredictionHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
              "    (act): SiLU()\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (drop): Dropout(p=0.0, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=14, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Bioclinical ModernBERT model and moving it to GPU for processing."
      ],
      "metadata": {
        "id": "eqXSehZI9BoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters,lr=6e-6)"
      ],
      "metadata": {
        "id": "UrNOGBtmT7KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring the AdamW optimizer. AdamW is chosen as it is a better version of Adam optimizer, where the weight decay and gradient are decoupled which improves performance.\n",
        "\n",
        "bias, gamma and beta have weight decay rate as 0 as they will not contribute to overfitting.\n",
        "Every other parameter has a weight decay of 0.01.\n",
        "\n",
        "The learning rate of the AdamW optimmizer is 0.000006. A small value is used because the model is being fine-tuned."
      ],
      "metadata": {
        "id": "lHiVGkBP9Who"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "imnWJ98RT7MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the device to GPU (cuda)."
      ],
      "metadata": {
        "id": "Wq0z8snn9W_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''checkpoint = torch.load('/content/drive/My Drive/checkpoints/epoch_2.pt')\n",
        "\n",
        "start_epoch = checkpoint['epoch']\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])'''"
      ],
      "metadata": {
        "id": "p8n-Oqn_CmJq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9a7c5a2e-d87f-4eb8-82bc-2e8584454762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"checkpoint = torch.load('/content/drive/My Drive/checkpoints/epoch_2.pt')\\n\\nstart_epoch = checkpoint['epoch']\\n\\nmodel.load_state_dict(checkpoint['model_state_dict'])\\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a checkpoint from a previous training run (in case the training was interrupted abruptly).\n",
        "Epoch number, model state dictionary and optimizer state dictionary are restored."
      ],
      "metadata": {
        "id": "W47XjuCq9X3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_set = []\n",
        "val_f1_accuracy_list,val_flat_accuracy_list,training_loss_list,epochs_list=[],[],[],[]\n",
        "\n",
        "# Number of training epochs (recommend between 5 and 10)\n",
        "epochs = 6\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "#for epoch in trange(start_epoch, epochs+1, desc=\"Remaining Epoch\"):\n",
        "\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0 #running loss\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels= batch\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass for multilabel classification\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "        loss_func = BCEWithLogitsLoss()\n",
        "        loss = loss_func(logits.view(-1,14),b_labels.type_as(logits).view(-1,14)) #convert labels to float for calculation\n",
        "\n",
        "        train_loss_set.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    current_avg_train_loss = tr_loss / nb_tr_steps\n",
        "    print(\"\\nTrain loss: \", current_avg_train_loss)\n",
        "    training_loss_list.append(current_avg_train_loss)\n",
        "\n",
        "    ###############################################################################\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    # Put model in evaluation mode to evaluate loss on the validation set\n",
        "    model.eval()\n",
        "\n",
        "    # Variables to gather full output\n",
        "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
        "\n",
        "    # Predict\n",
        "    for i, batch in enumerate(validation_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            b_logit_pred = outs[0]\n",
        "            pred_label = torch.sigmoid(b_logit_pred)\n",
        "\n",
        "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
        "        pred_label = pred_label.to('cpu').numpy()\n",
        "        b_labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tokenized_texts.append(b_input_ids)\n",
        "    logit_preds.append(b_logit_pred)\n",
        "    true_labels.append(b_labels)\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "    # Flatten outputs\n",
        "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
        "    true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    threshold = 0.50\n",
        "    pred_bools = [pl>threshold for pl in pred_labels]\n",
        "    true_bools = [tl==1 for tl in true_labels]\n",
        "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
        "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
        "\n",
        "    print('F1 Validation Accuracy:', val_f1_accuracy)\n",
        "    print('Flat Validation Accuracy:', val_flat_accuracy, \"\\n\")\n",
        "    val_f1_accuracy_list.append(val_f1_accuracy)\n",
        "    val_flat_accuracy_list.append(val_flat_accuracy)\n",
        "    epochs_list.append(epochs)\n",
        "\n",
        "    checkpoint_path = os.path.join('/content/drive/My Drive/checkpoints', f'epoch_{epoch+1}.pt')\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': current_avg_train_loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"\\nCheckpoint saved at {checkpoint_path}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSdNequkYNxN",
        "outputId": "0661afbc-10ff-443c-bde4-89fd149381b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/6 [00:00<?, ?it/s]W0708 10:55:28.535000 503 torch/_inductor/utils.py:1137] [1/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss:  0.3495073156251061\n",
            "F1 Validation Accuracy: 81.17647058823529\n",
            "Flat Validation Accuracy: 12.068965517241379 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  17%|█▋        | 1/6 [16:40<1:23:22, 1000.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved at /content/drive/My Drive/checkpoints/epoch_1.pt\n",
            "\n",
            "\n",
            "Train loss:  0.29594874388027576\n",
            "F1 Validation Accuracy: 83.43023255813954\n",
            "Flat Validation Accuracy: 13.793103448275861 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  33%|███▎      | 2/6 [32:58<1:05:48, 987.15s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved at /content/drive/My Drive/checkpoints/epoch_2.pt\n",
            "\n",
            "\n",
            "Train loss:  0.2778746721545054\n",
            "F1 Validation Accuracy: 84.43804034582134\n",
            "Flat Validation Accuracy: 17.24137931034483 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 3/6 [49:14<49:06, 982.29s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved at /content/drive/My Drive/checkpoints/epoch_3.pt\n",
            "\n",
            "\n",
            "Train loss:  0.263462400003787\n",
            "F1 Validation Accuracy: 84.6820809248555\n",
            "Flat Validation Accuracy: 13.793103448275861 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 4/6 [1:05:02<32:17, 968.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved at /content/drive/My Drive/checkpoints/epoch_4.pt\n",
            "\n",
            "\n",
            "Train loss:  0.24834384531864234\n",
            "F1 Validation Accuracy: 85.29411764705883\n",
            "Flat Validation Accuracy: 17.24137931034483 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  83%|████████▎ | 5/6 [1:21:00<16:04, 964.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved at /content/drive/My Drive/checkpoints/epoch_5.pt\n",
            "\n",
            "\n",
            "Train loss:  0.23039598285310692\n",
            "F1 Validation Accuracy: 86.45533141210375\n",
            "Flat Validation Accuracy: 17.24137931034483 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 6/6 [1:36:54<00:00, 969.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved at /content/drive/My Drive/checkpoints/epoch_6.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training runs for 6 epochs. The model is trained on batches of data from the train tensor dataset using the train dataloader.\n",
        "Loss is calculated using BCEWithLogits loss. It is chosen for multi-label classification as it considers each label independently, any relationships between labels will not be considered.\n",
        "After each epoch, the model is evaluated on validation data, then F1 score and accuracy is calculated.\n",
        "Checkpoints are implemented by saving the model and optimizer state dictionaries of each epoch to Google Drive."
      ],
      "metadata": {
        "id": "bOfZKDbeEzK3"
      }
    }
  ]
}