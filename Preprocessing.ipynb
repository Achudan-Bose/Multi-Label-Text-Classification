{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQPgxIvuJ45hDybIHYbUXK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpCkjnmrL9vV",
        "outputId": "72d10095-93b7-4189-fb70-783861cf465e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import TensorDataset"
      ],
      "metadata": {
        "id": "TW3z0J2YMJqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/PubMed Multi Label Text Classification Dataset Processed.csv')"
      ],
      "metadata": {
        "id": "sT0rYpUIMC_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop_duplicates()"
      ],
      "metadata": {
        "id": "liWR4B6LMngD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping duplicate values as they would be noise during training."
      ],
      "metadata": {
        "id": "xjoryAfAXbPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rowsums=df.iloc[:,6:].sum(axis=1)\n",
        "no_label_count = 0\n",
        "for sum in rowsums.values:\n",
        "    if sum==0:\n",
        "        no_label_count +=1\n",
        "print(\"Total number of articles without label:\", no_label_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eqXTCnaMnjN",
        "outputId": "8cdd4ee0-d62a-4c0f-a3c0-f4ba45010b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of articles without label: 361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Articles without any label (all label values of a row are 0) would be noise during training. To find the number of rows with no labels, the row sum is calculated for all rows, across the columns starting from 7th column (position 6). A loop iterates through rowsums, where if the value is 0, the count increases by 1."
      ],
      "metadata": {
        "id": "bUEVOVipXfsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_row=[]\n",
        "for sum in rowsums.values:\n",
        "    check_row.append(sum)\n",
        "df['check']=check_row\n",
        "df=df.drop(df[df['check']==0].index)\n",
        "df=df.drop(['check'],axis=1)\n",
        "print(\"Removed articles without label\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIfTEsMRNrvk",
        "outputId": "6919da6c-a0ce-40bd-b005-81f604017d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed articles without label\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-2744747810.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['check']=check_row\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove the articles with no label, the row sums are added as a column to the dataframe so that a condition (to check if rowsums is 0) can be defined using this column to drop the rows where row sum is 0."
      ],
      "metadata": {
        "id": "ayANOhUDZvoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols=list(df.columns)\n",
        "mesh_Heading_categories=cols[6:]\n",
        "num_labels=len(mesh_Heading_categories)\n",
        "print('Root Labels:',mesh_Heading_categories)\n",
        "print('Number of Labels:' ,num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRdYniRJNryq",
        "outputId": "8b1bbc56-86ae-4e83-e039-1e7545742a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Labels: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'Z']\n",
            "Number of Labels: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the label names and number of labels, column names are stored as a list, then the list is sliced from index 6 (to the last index) and stored in another list, the length of which is calculated to get the number of labels."
      ],
      "metadata": {
        "id": "KruVn2xlz_MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df, random_state=32, test_size=0.20, shuffle=True)\n",
        "\n",
        "print(df_train.shape)\n",
        "print(df_test.shape)"
      ],
      "metadata": {
        "id": "ROXmMZ5xNr1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39286de8-ebe5-464b-cb93-2af1ec900af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(39650, 20)\n",
            "(9913, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into train and test data in an 80:20 split. The data is shuffled so that train and test data properly represent the whole data (no underlying patterns in the order of data). Random state ensures that the data is split in the exact same way whenever the split is done."
      ],
      "metadata": {
        "id": "fQCQ3dL61Llc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['one_hot_labels'] = list(df_train[mesh_Heading_categories].values)\n",
        "\n",
        "labels = list(df_train.one_hot_labels.values)\n",
        "Article_train = list(df_train.abstractText.values)"
      ],
      "metadata": {
        "id": "_GQ2aqY_MnlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a new column of one hot encoded values of labels in the training dataframe, the values of the labels are extracted and converted to a list. The elements of the list are assigned as values to the column.\n",
        "\n",
        "The articles and the one hot encoded values of training data are stored as lists as they will be the independent and target variables."
      ],
      "metadata": {
        "id": "D0KzaZ8M5E0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "tokenizer=AutoTokenizer.from_pretrained('thomas-sounack/BioClinical-ModernBERT-base', do_lower_case=True)\n",
        "\n",
        "encodings=tokenizer.batch_encode_plus(Article_train,max_length=max_length,padding=True,truncation=True)\n",
        "print('tokenizer outputs: ', encodings.keys())\n",
        "\n",
        "input_ids=encodings['input_ids']\n",
        "attention_masks=encodings['attention_mask']"
      ],
      "metadata": {
        "id": "i0UYz38PM4Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94:\n",
        "UserWarning:\n",
        "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
        "\n",
        "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
        "\n",
        "You will be able to reuse this secret in all of your notebooks.\n",
        "\n",
        "Please note that authentication is recommended but still optional to access public models or datasets.\n",
        "  warnings.warn(\n",
        "\n",
        "tokenizer_config.json:\n",
        " 20.8k/? [00:00<00:00, 872kB/s]\n",
        "\n",
        "tokenizer.json:\n",
        " 2.13M/? [00:00<00:00, 28.3MB/s]\n",
        "\n",
        "special_tokens_map.json: 100%\n",
        " 693/693 [00:00<00:00, 51.2kB/s]\n",
        "\n",
        "tokenizer outputs:  KeysView({'input_ids': [[50281, 1231, 6266, 11249, 253, 2219, 273, 767, 10652, 1363, 1119, 281, ...."
      ],
      "metadata": {
        "id": "vNnPjEdQAIaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max length is a hyperparameter which defines the max token sequence length. I tried some combinations of max length and batch size (hyperparameter during loading the data in batches) and got OOM error for (256,64), (256,32), so (128,64) seemed appropriate.\n",
        "\n",
        "A pretrained tokenizer is loaded for the tokenization of articles so that it is in the correct format of input for the transformer. The articles are converted to lower case before tokenization.\n",
        "\n",
        "Tokenization is done on a batch of data, and not sequentially. If a token sequence is shorter than max length, padding is added and if a token sequence is longer than max length, then it is truncated.\n",
        "\n",
        "Input IDs are the numerical representation of token sequences and attention masks define what tokens to consider and what tokens to ignore (padding tokens)."
      ],
      "metadata": {
        "id": "HSk8l8yk_ZlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks, random_state=2020, test_size=0.20)\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "metadata": {
        "id": "fMHkRdwGM4LP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e635194-ea7f-4796-8705-9f265f33bfe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-1352811930.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  train_labels = torch.tensor(train_labels)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input IDs, labels (one hot encoded values) and attention masks are split into training and validation data in an 80:20 split. All training and validation data is converted into torch tensors, which is the standard data structure for deep learning."
      ],
      "metadata": {
        "id": "ZrAPb1PoIaNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=TensorDataset(train_inputs, train_masks, train_labels,)\n",
        "\n",
        "validation_data=TensorDataset(validation_inputs, validation_masks, validation_labels,)\n",
        "\n",
        "torch.save(validation_data,'/content/drive/My Drive/tensor data/validation_data.pt')\n",
        "torch.save(train_data,'/content/drive/My Drive/tensor data/train_data.pt')"
      ],
      "metadata": {
        "id": "w5pAAP8TM4Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and validation tensors are stored in tensor datasets as tuples. Tensor datasets make it easier to manage data to be loaded by the dataloaders.\n",
        "\n",
        "Tensor datasets are saved in Google Drive, to be used in Training notebook."
      ],
      "metadata": {
        "id": "t9d77cVwIat4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['one_hot_labels'] = list(df_test[mesh_Heading_categories].values)\n",
        "\n",
        "test_labels = list(df_test.one_hot_labels.values)\n",
        "Articles_test = list(df_test.abstractText.values)"
      ],
      "metadata": {
        "id": "oyrC61j0U7A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a new column of one hot encoded values of labels in the tes dataframe, the values of the labels are extracted and converted to a list. The elements of the list are assigned as values to the column.\n",
        "\n",
        "The articles and the one hot encoded values of test data are stored as lists as they will be the independent and target variables."
      ],
      "metadata": {
        "id": "BF-SDaElIbAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = tokenizer.batch_encode_plus(Articles_test,max_length=max_length,padding=True,truncation=True)\n",
        "test_input_ids = test_encodings['input_ids']\n",
        "test_attention_masks = test_encodings['attention_mask']"
      ],
      "metadata": {
        "id": "dtEaKkaJU7DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization of the test data, input IDs and attention masks are extracted."
      ],
      "metadata": {
        "id": "cGYzRJCRIbat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "test_masks = torch.tensor(test_attention_masks)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels,)\n",
        "\n",
        "torch.save(test_data,'/content/drive/My Drive/tensor data/test_data.pt')"
      ],
      "metadata": {
        "id": "MwT5ysPnU7Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test tensors are stored in a tensor dataset as tuples. Tensor dataset is stored in Google Drive, to be used in Evaluation Notebook."
      ],
      "metadata": {
        "id": "SnK7j_53IcDU"
      }
    }
  ]
}